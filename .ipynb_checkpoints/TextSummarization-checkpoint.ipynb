{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main goal:\n",
    " - summary the indonesia text using machine learning approach\n",
    "\n",
    "Dataset:\n",
    " - twitter\n",
    " - news\n",
    " - blogger\n",
    "\n",
    "Steps:\n",
    " 1. pre-processing:\n",
    "    - colletion data and cleansing the data\n",
    "    - reduce the problem dimensionality (stop words, stemming, features selection, etc.)\n",
    " 2. dataset modeling: \n",
    "    - deciding how to construct training datadatset. \n",
    " 3. Analysis: \n",
    "    - summary indonesian text using one or more algorithm. many options: Reinforcement Algorithm (Jiwanggi & Adriani, 2016), k-nn, SVM, Naive Bayes, Neural network\n",
    "\n",
    "\n",
    "Referencess:\n",
    " Summary indonesian text\n",
    "  - Meganingrum Arista Jiwanggi, Mirna Adriani. 2016, \"Topic Summarization of Microblog Document in Bahasa Indonesia using the Phrase Reinforcement Algorithm\"\n",
    "\n",
    " Pre-processing models (OPTIONS)\n",
    "  - Mirna Adriani, Jelita Asian, Bobby Nazief, Seyed MM Tahaghoghi, Hugh E Williams. 2007, \"Stemming Indonesian: A confix-stripping approach\" \n",
    "  - Mirna Adriani. 2000, \"Using statistical term similarity for sense disambiguation in cross-language information retrieval\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Komponen yang dibuat sendiri:\n",
    "1. Wordnet (Sinonim)\n",
    "2. NLG (buat ngerangkai kalimat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "path = './dataset'\n",
    "dataset = []\n",
    " \n",
    "directories = os.listdir(path)\n",
    "for dir_name in directories:\n",
    "    path_file = path + '/' + dir_name\n",
    "    files = os.listdir(path_file)\n",
    "    for file_name in files:\n",
    "        path_to_open = path_file + '/' + file_name\n",
    "        with open(path_to_open, 'r') as f:\n",
    "            tweet_info = {}\n",
    "            read_file = json.loads(f.read())\n",
    "            tweet_info['trending_topic'] = dir_name\n",
    "            tweet_info['tweet'] = read_file['text']\n",
    "            dataset.append(tweet_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataset(dataset):\n",
    "    dataset.to_csv('twitter_data.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1857, 2)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_tweets = pd.DataFrame(dataset)\n",
    "df_tweets_train = df_tweets.copy()\n",
    "df_tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "#JanganSuriahkanIndonesia    1857\n",
       "Name: trending_topic, dtype: int64"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets['trending_topic'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# JanganSuriahkanIndonesia'"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets['tweet'].iloc[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing\n",
    "https://github.com/riochr17/Named-Entity-Tagger-ID\n",
    "<br>\n",
    "https://github.com/lilia-simeonova/preprocessing-tweets\n",
    "<br>\n",
    "http://iopscience.iop.org/article/10.1088/1742-6596/801/1/012072/pdf\n",
    "<br>\n",
    "http://taufiksutanto.blogspot.com/2018/01/tokenization-dalam-bahasa-inggris.html\n",
    "<br>\n",
    "http://www.yasirblog.com/2017/05/normalisasi-data-text-text.html\n",
    "<br>\n",
    "http://norvig.com/spell-correct.html\n",
    "<br>\n",
    "https://github.com/sastrawi/nlp-bahasa-indonesia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py_translator import Translator\n",
    "import enchant\n",
    "\n",
    "def translate(text):\n",
    "    counter = 1;\n",
    "    checker = enchant.Dict(\"en_US\")\n",
    "    split_text = text.split(' ')\n",
    "    words = []\n",
    "    translator = Translator()\n",
    "    for word in split_text:\n",
    "        if word != '':\n",
    "            if checker.check(word) and word != 'twitter':\n",
    "                translated_word = translator.translate(text=word, dest='id', src='en').text \n",
    "                words.append(translated_word)\n",
    "                if counter > 10:\n",
    "                    delay_request(5,7)\n",
    "                    counter = 1\n",
    "            else:\n",
    "                words.append(word.replace(' ',''))\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "import random\n",
    "\n",
    "def delay_request(lower, upper):\n",
    "    print(random.uniform(lower, upper))\n",
    "    sleep(random.uniform(lower, upper))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_url(text):\n",
    "    return re.sub(r'http\\S+', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_text_to_lowercase(text):\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(text):\n",
    "    return re.sub(r'(^|\\s)(:D|:\\/|:\\)+|;\\)|:-\\))(?=\\s|[^[:alnum:]+-]|$)', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def spell_checker(text):\n",
    "    words = text.split(' ')\n",
    "    with open('spellchecker.json') as file_opened:\n",
    "        text = file_opened.read()\n",
    "    word_checker = json.loads(text)\n",
    "    for idx in range(len(words)):\n",
    "        if words[idx] in word_checker:\n",
    "            words[idx] = word_checker[words[idx]]\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def substitue_character(text):\n",
    "    new_text = text.encode('ascii',errors='ignore').decode('utf-8', 'ignore')\n",
    "    new_text = new_text.replace('\\n', ' ')\n",
    "    new_text = new_text.replace('?', ' ')\n",
    "    new_text = new_text.replace('\"', '')\n",
    "    new_text = new_text.replace(\"'\", '')\n",
    "    new_text = re.sub(r'[.,\\/!$%\\^&\\*;:{}=\\`~()]', ' ', new_text)\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dwilingga(text):\n",
    "    match = re.match(r'\\w*(?=2(nya)?)', text)\n",
    "    if match is not None:\n",
    "        words = text.split(' ')\n",
    "        for idx in range(len(words)):\n",
    "            match_word = re.match(r'\\w*(?=2(nya)?)', words[idx])\n",
    "            if match_word is not None:\n",
    "                words[idx] = words[idx].replace('2', '-' + match_word.group())\n",
    "        return \" \".join(words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_code_word(text):\n",
    "    match = re.search(r'(?:[\\d]+[\\w]|[\\w]+[\\d])[\\w\\d]*', text)\n",
    "    if match is not None:\n",
    "        words = text.split(' ')\n",
    "        for idx in range(len(words)):\n",
    "            match_word = re.match(r'(?:[\\d]+[\\w]|[\\w]+[\\d])[\\w\\d]*', words[idx])\n",
    "            match_hastag = re.match(r'(#[\\w\\d]*)', words[idx])\n",
    "            if match_word is not None and match_hastag is None:\n",
    "                words[idx] = ''\n",
    "        return \" \".join(words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prefix_fixing(text):\n",
    "    match = re.search(r'\\b[bkmpst][bcdfghjklmnpqrstvwxyz]', text)\n",
    "    words = text.split(' ')\n",
    "    if match is not None:\n",
    "        for idx in range(len(words)):\n",
    "            match_word = re.search(r'\\b[bkmpst][bcdfghjklmnpqrstvwxyz]', words[idx])\n",
    "            if match_word is not None:\n",
    "                words[idx] = words[idx][0] + 'e' + words[idx][1:]\n",
    "    match3 = re.search(r'\\b[p][bcdefghjklmnpqstvwxyz]', text)\n",
    "    if match3 is not None:\n",
    "        for idx in range(len(words)):\n",
    "            match_word = re.search(r'\\b[p][bcdfghjklmnpqstvwxyz]', words[idx])\n",
    "            if match_word is not None:\n",
    "                words[idx] = words[idx][0] + 'i' + words[idx][1:]\n",
    "    match2 = re.search(r'\\b[d][bcdefghjklmnpqrstvwxyz]', text)\n",
    "    if match2 is not None:\n",
    "        for idx in range(len(words)):\n",
    "            match_word = re.search(r'\\b[d][bcdfghjklmnpqrstvwxyz]', words[idx])\n",
    "            if match_word is not None:\n",
    "                words[idx] = words[idx][0] + 'i' + words[idx][1:]\n",
    "    return \" \".join(words) if match is not None or match2 is not None or match3 is not None else text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_word(text):\n",
    "    match = re.match(r'(?!ke)\\d*\\b', text)\n",
    "    number_dict = {\n",
    "        '1': 'satu',\n",
    "        '2': 'dua',\n",
    "        '3': 'tiga',\n",
    "        '4': 'empat',\n",
    "        '5': 'lima',\n",
    "        '6': 'enam',\n",
    "        '7': 'tujuh',\n",
    "        '8': 'delapan',\n",
    "        '9': 'sembilan'\n",
    "    }\n",
    "    if match is not None:\n",
    "        words = text.split(' ')\n",
    "        for idx in range(len(words)):\n",
    "            match_word = re.match(r'(?!ke)\\d*\\b', words[idx])\n",
    "            if match_word is not None:\n",
    "                words[idx] = words[idx].replace('2', '-' + match_word.group())\n",
    "        return \" \".join(words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_mention(text):\n",
    "    return re.sub(r'@[\\w\\d_]+', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_to_many_character(text):\n",
    "    list_of_char = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
    "    list_of_vocal = ['a', 'i', 'u', 'e', 'o']\n",
    "    new_text = text\n",
    "    for idx in range(len(list_of_vocal)):\n",
    "        for jdx in range(idx, len(list_of_vocal)):\n",
    "            pattern = r'\\b' + list_of_vocal[idx] + list_of_vocal[jdx] + r'\\b'\n",
    "            pattern_2 = r'\\b' + list_of_vocal[jdx] + list_of_vocal[idx] + r'\\b'\n",
    "            pattern_3 = r'\\b[bcdfghjklmnpqrstvwxyz][bcdfghjklmnpqrstvwxyz]\\b'\n",
    "            new_text = re.sub(pattern, '', new_text)\n",
    "            new_text = re.sub(pattern_2, '', new_text)\n",
    "            new_text = re.sub(pattern_3, '', new_text)\n",
    "    for character in list_of_char:\n",
    "        pattern = character + r'{3,}'\n",
    "        pattern_2 = character + r'{2,}\\b'\n",
    "        new_text = re.sub(pattern, character, new_text)\n",
    "        new_text = re.sub(pattern_2, character, new_text)\n",
    "    new_text = re.sub(r'(wk){2,}', '', new_text)\n",
    "    new_text = re.sub(r'(ha){2,}', '', new_text)\n",
    "    new_text = re.sub(r'(he){2,}', '', new_text)\n",
    "    new_text = re.sub(r'(hi){2,}', '', new_text)\n",
    "    new_text = re.sub(r'(bla){2,}', '', new_text)\n",
    "    new_text = re.sub(r'\\b[a-z]\\b', '', new_text)\n",
    "    new_text = re.sub(r'\\b(kw)\\b', '', new_text)\n",
    "    new_text = re.sub(r'\\b(ha)\\b', '', new_text)\n",
    "    new_text = re.sub(r'\\b(he)\\b', '', new_text)\n",
    "    new_text = re.sub(r'\\b(bla)\\b', '', new_text)\n",
    "    new_text = re.sub(r'\\b(hi)\\b', '', new_text)\n",
    "    new_text = re.sub(r'\\b(co)\\b', '', new_text)\n",
    "    new_text = re.sub(r'\\b(id)\\b', '', new_text)\n",
    "    new_text = re.sub(r'\\b(com)\\b', '', new_text)\n",
    "    new_text = re.sub(r'\\b(org)\\b', '', new_text)\n",
    "    new_text = re.sub(r'\\b(net)\\b', '', new_text)\n",
    "    new_text = re.sub(r'\\b(gov)\\b', '', new_text)\n",
    "    new_text = remove_mention(new_text)\n",
    "    new_text = remove_code_word(new_text)\n",
    "    new_text = dwilingga(new_text)\n",
    "    new_text = rank_word(new_text)\n",
    "    new_text = prefix_fixing(new_text)\n",
    "    new_text = re.sub(r' {2,}', ' ', new_text)\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "\n",
    "def remove_stop_words(sentence):\n",
    "    factory = StopWordRemoverFactory().create_stop_word_remover()\n",
    "    return factory.remove(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "def stemming(sentence):\n",
    "    factory = StemmerFactory()\n",
    "    stemmer = factory.create_stemmer()\n",
    "    return stemmer.stem(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    new_text = substitue_character(text)\n",
    "    new_text = convert_text_to_lowercase(new_text)\n",
    "    new_text = remove_url(new_text)\n",
    "    new_text = remove_emoji(new_text)\n",
    "    new_text = cut_to_many_character(new_text)\n",
    "#     new_text = translate(new_text)\n",
    "    new_text = remove_stop_words(new_text)\n",
    "    new_text = spell_checker(new_text)\n",
    "    new_text = stemming(new_text)\n",
    "    new_text = new_text.replace('#', '')\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_train['tweet'] = df_tweets_train['tweet'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jangansuriahkanindonesia moga indonesia aman umat islam moga satu adu domba nama apa sesal negri beranta jangansuriahkanindonesia'"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets_train['tweet'].iloc[456]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# JanganSuriahkanIndonesia  ya! Semoga Indonesia tetap aman, ummat Islam semoga tetap bersatu, jangan mau diadu domba atas nama apapun !!! Jangan sampai nanti kita menyesal setelah negri kita berantakan #JanganSuriahkanIndonesia'"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets['tweet'].iloc[456]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = []\n",
    "for text in df_tweets_train['tweet']:\n",
    "    split_text = text.split(' ')\n",
    "    for word in split_text:\n",
    "        dummy.append(word)\n",
    "unique = set(dummy)\n",
    "text_list = {}\n",
    "for word in unique:\n",
    "    text_list[word] =''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('uniquejson.json', '+w') as opened_file:\n",
    "    json.dump(text_list, opened_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_frequencies_list(tweets):\n",
    "    word_frequencies = []\n",
    "    word_dict = []\n",
    "    for tweet in tweets:\n",
    "        words_in_sentence = tweet.split(' ')\n",
    "        for word in words_in_sentence:\n",
    "            if word in word_dict:\n",
    "                word_frequencies[word_dict.index(word)] += 1\n",
    "            else:\n",
    "                word_frequencies.append(1)\n",
    "                word_dict.append(word)\n",
    "    return word_dict, word_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_word_list(word_dict, word_frequencies, treshold=2):\n",
    "    new_word_frequencies = word_frequencies.copy()\n",
    "    new_word_dict = word_dict.copy()\n",
    "    idx = 0;\n",
    "    while idx < len(new_word_frequencies):\n",
    "        if new_word_frequencies[idx] < treshold:\n",
    "            new_word_dict.pop(idx)\n",
    "            new_word_frequencies.pop(idx)\n",
    "        else:\n",
    "            idx += 1\n",
    "    return new_word_dict, new_word_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_frequencies_list_all(tweets):\n",
    "    graph = []\n",
    "    graph_idx = 0\n",
    "    for tweet in tweets:\n",
    "        words_in_sentence = tweet.split(' ')\n",
    "        prev_idx = -1\n",
    "        idx_now = 0\n",
    "        for word in words_in_sentence:\n",
    "            if word != '':\n",
    "                is_not_in_graph = True\n",
    "                for idx_graph in range(len(graph)):\n",
    "                    if graph[idx_graph]['word'] == word:\n",
    "                        graph[idx_graph]['counter'] += 1\n",
    "                        is_not_in_graph = False\n",
    "                        idx_graph_representation = idx_graph\n",
    "                        break\n",
    "                if is_not_in_graph:\n",
    "                    node = {\n",
    "                        'idx': graph_idx,\n",
    "                        'word': word,\n",
    "                        'counter': 1,\n",
    "                        'next_neighbours': [],\n",
    "                        'prev_neighbours': []\n",
    "                    }\n",
    "                    if prev_idx != -1:\n",
    "                        node['prev_neighbours'].append(prev_idx)\n",
    "                        if graph_idx not in graph[prev_idx]['next_neighbours'] and graph_idx != prev_idx:\n",
    "                            graph[prev_idx]['next_neighbours'].append(graph_idx)\n",
    "                    prev_idx = graph_idx\n",
    "                    graph_idx += 1\n",
    "                    graph.append(node)\n",
    "                else:\n",
    "                    if prev_idx != -1:\n",
    "                        if prev_idx not in graph[idx_graph_representation]['prev_neighbours'] and idx_graph_representation != prev_idx:\n",
    "                            graph[idx_graph_representation]['prev_neighbours'].append(prev_idx)\n",
    "                        if idx_graph_representation not in graph[prev_idx]['next_neighbours'] and idx_graph_representation != prev_idx:\n",
    "                            graph[prev_idx]['next_neighbours'].append(idx_graph_representation)\n",
    "                    prev_idx = idx_graph_representation\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_frequencies_list_reduce(tweets, treshold=2):\n",
    "    words, freqs = word_frequencies_list(tweets)\n",
    "    new_words, new_freqs = reduce_word_list(words, freqs, treshold)\n",
    "    graph = []\n",
    "    graph_idx = 0\n",
    "    for tweet in tweets:\n",
    "        words_in_sentence = tweet.split(' ')\n",
    "        prev_idx = -1\n",
    "        idx_now = 0\n",
    "        for word in words_in_sentence:\n",
    "            if word != '' and word in new_words:\n",
    "                is_not_in_graph = True\n",
    "                for idx_graph in range(len(graph)):\n",
    "                    if graph[idx_graph]['word'] == word:\n",
    "                        graph[idx_graph]['counter'] += 1\n",
    "                        is_not_in_graph = False\n",
    "                        idx_graph_representation = idx_graph\n",
    "                        break\n",
    "                if is_not_in_graph:\n",
    "                    node = {\n",
    "                        'idx': graph_idx,\n",
    "                        'word': word,\n",
    "                        'counter': 1,\n",
    "                        'next_neighbours': [],\n",
    "                        'prev_neighbours': []\n",
    "                    }\n",
    "                    if prev_idx != -1:\n",
    "                        node['prev_neighbours'].append(prev_idx)\n",
    "                        if graph_idx not in graph[prev_idx]['next_neighbours'] and graph_idx != prev_idx:\n",
    "                            graph[prev_idx]['next_neighbours'].append(graph_idx)\n",
    "                    prev_idx = graph_idx\n",
    "                    graph_idx += 1\n",
    "                    graph.append(node)\n",
    "                else:\n",
    "                    if prev_idx != -1:\n",
    "                        if prev_idx not in graph[idx_graph_representation]['prev_neighbours'] and idx_graph_representation != prev_idx:\n",
    "                            graph[idx_graph_representation]['prev_neighbours'].append(prev_idx)\n",
    "                        if idx_graph_representation not in graph[prev_idx]['next_neighbours'] and idx_graph_representation != prev_idx:\n",
    "                            graph[prev_idx]['next_neighbours'].append(idx_graph_representation)\n",
    "                    prev_idx = idx_graph_representation\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def calculate_weight(counter, distance, base):\n",
    "    return counter - (distance * (math.log(counter)/math.log(base)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_path(idx, graph, distance, base, data={'path': [], 'weight': 0}, iterator=5):\n",
    "    if len(graph[idx]['next_neighbours']) == 0 or iterator == distance:\n",
    "        new_data = {\n",
    "            'path': data['path'].copy(),\n",
    "            'weight': data['weight']\n",
    "        }\n",
    "        if idx not in data['path']:\n",
    "            new_data['path'].append(idx)\n",
    "            new_data['weight'] += calculate_weight(graph[idx]['counter'], distance, base)\n",
    "        return new_data\n",
    "    else:\n",
    "        new_data = {\n",
    "            'path': data['path'].copy(),\n",
    "            'weight': data['weight']\n",
    "        }\n",
    "        if idx not in data['path']:\n",
    "            new_data['path'].append(idx)\n",
    "            new_data['weight'] += calculate_weight(graph[idx]['counter'], distance, base)\n",
    "            neighbours = []\n",
    "            for next_idx in graph[idx]['next_neighbours']:\n",
    "                data_send = {\n",
    "                    'path': new_data['path'].copy(),\n",
    "                    'weight': data['weight']\n",
    "                }\n",
    "                if next_idx not in data_send['path']:\n",
    "                    neighbours.append(get_next_path(next_idx, graph, distance + 1, base, data_send, iterator))\n",
    "            data_max = {'path': [], 'weight': 0}\n",
    "            for neighbour in neighbours:\n",
    "                if neighbour['weight'] > data_max['weight']:\n",
    "                    data_max = neighbour.copy()\n",
    "            return data_max.copy()\n",
    "        else:\n",
    "            return new_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prev_path(idx, graph, distance, base, data={'path': [], 'weight': 0}, iterator=5):\n",
    "    if len(graph[idx]['next_neighbours']) == 0 or iterator == distance:\n",
    "        new_data = {\n",
    "            'path': data['path'].copy(),\n",
    "            'weight': data['weight']\n",
    "        }\n",
    "        if idx not in data['path']:\n",
    "            new_data['path'].insert(0, idx)\n",
    "            new_data['weight'] += calculate_weight(graph[idx]['counter'], distance, base)\n",
    "        return new_data\n",
    "    else:\n",
    "        new_data = {\n",
    "            'path': data['path'].copy(),\n",
    "            'weight': data['weight']\n",
    "        }\n",
    "        if idx not in data['path']:\n",
    "            new_data['path'].insert(0, idx)\n",
    "            new_data['weight'] += calculate_weight(graph[idx]['counter'], distance, base)\n",
    "            neighbours = []\n",
    "            for next_idx in graph[idx]['prev_neighbours']:\n",
    "                data_send = {\n",
    "                    'path': new_data['path'].copy(),\n",
    "                    'weight': data['weight']\n",
    "                }\n",
    "                if next_idx not in data_send['path']:\n",
    "                    neighbours.append(get_prev_path(next_idx, graph, distance + 1, base, data_send, iterator))\n",
    "            data_max = {'path': [], 'weight': 0}\n",
    "            for neighbour in neighbours:\n",
    "                if neighbour['weight'] > data_max['weight']:\n",
    "                    data_max = neighbour.copy()\n",
    "            return data_max.copy()\n",
    "        else:\n",
    "            return new_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_index(graph, topic):\n",
    "    for node in graph:\n",
    "        if node['word'] == topic:\n",
    "            return node['idx']\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_path(path_next, path_prev):\n",
    "    all_path = {\n",
    "        'path': path_prev['path'][:-1] + path_next['path'],\n",
    "        'weight': path_prev['weight'] + path_next['weight']\n",
    "    }\n",
    "    return all_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence(graph, path):\n",
    "    sentence = ''\n",
    "    for item in path['path']:\n",
    "        for node in graph:\n",
    "            if node['idx'] == item:\n",
    "                sentence += node['word'] + ' '\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(tweets, topic, base=2, iterator=5, treshold=10):\n",
    "    graph = graph_frequencies_list_reduce(tweets, treshold)\n",
    "    topic_index = get_topic_index(graph, topic)\n",
    "    if topic_index == -1:\n",
    "        return 'this string cannot be a topic'\n",
    "    path_next = get_next_path(topic_index, graph, 0, base, iterator=iterator)\n",
    "    path_prev = get_prev_path(topic_index, graph, 0, base, iterator=iterator)\n",
    "    all_path = combine_path(path_next, path_prev)\n",
    "    return get_sentence(graph, all_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [],
   "source": [
    "trendinc_topic = df_tweets_train['trending_topic'].iloc[0].replace('#', '').lower()\n",
    "sentence = summarize(df_tweets_train['tweet'], trendinc_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "twitter muslim negara kehilafah indonesia jangansuriahkanindonesia hoax alhamdulillah ga bangsa twitter \n"
     ]
    }
   ],
   "source": [
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
