{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main goal:\n",
    " - summary the indonesia text using machine learning approach\n",
    "\n",
    "Dataset:\n",
    " - twitter\n",
    " - news\n",
    " - blogger\n",
    "\n",
    "Steps:\n",
    " 1. pre-processing:\n",
    "    - colletion data and cleansing the data\n",
    "    - reduce the problem dimensionality (stop words, stemming, features selection, etc.)\n",
    " 2. dataset modeling: \n",
    "    - deciding how to construct training datadatset. \n",
    " 3. Analysis: \n",
    "    - summary indonesian text using one or more algorithm. many options: Reinforcement Algorithm (Jiwanggi & Adriani, 2016), k-nn, SVM, Naive Bayes, Neural network\n",
    "\n",
    "\n",
    "Referencess:\n",
    " Summary indonesian text\n",
    "  - Meganingrum Arista Jiwanggi, Mirna Adriani. 2016, \"Topic Summarization of Microblog Document in Bahasa Indonesia using the Phrase Reinforcement Algorithm\"\n",
    "\n",
    " Pre-processing models (OPTIONS)\n",
    "  - Mirna Adriani, Jelita Asian, Bobby Nazief, Seyed MM Tahaghoghi, Hugh E Williams. 2007, \"Stemming Indonesian: A confix-stripping approach\" \n",
    "  - Mirna Adriani. 2000, \"Using statistical term similarity for sense disambiguation in cross-language information retrieval\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Komponen yang dibuat sendiri:\n",
    "1. Wordnet (Sinonim)\n",
    "2. NLG (buat ngerangkai kalimat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "path = './dataset'\n",
    "dataset = []\n",
    " \n",
    "directories = os.listdir(path)\n",
    "for dir_name in directories:\n",
    "    path_file = path + '/' + dir_name\n",
    "    files = os.listdir(path_file)\n",
    "    for file_name in files:\n",
    "        path_to_open = path_file + '/' + file_name\n",
    "        with open(path_to_open, 'r') as f:\n",
    "            tweet_info = {}\n",
    "            read_file = json.loads(f.read())\n",
    "            tweet_info['trending_topic'] = dir_name\n",
    "            tweet_info['tweet'] = read_file['text']\n",
    "            dataset.append(tweet_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataset(dataset):\n",
    "    dataset.to_csv('twitter_data.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1857, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_tweets = pd.DataFrame(dataset)\n",
    "df_tweets_train = df_tweets.copy()\n",
    "df_tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "#JanganSuriahkanIndonesia    1857\n",
       "Name: trending_topic, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets['trending_topic'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# JanganSuriahkanIndonesia'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets['tweet'].iloc[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing\n",
    "https://github.com/riochr17/Named-Entity-Tagger-ID\n",
    "<br>\n",
    "https://github.com/lilia-simeonova/preprocessing-tweets\n",
    "<br>\n",
    "http://iopscience.iop.org/article/10.1088/1742-6596/801/1/012072/pdf\n",
    "<br>\n",
    "http://taufiksutanto.blogspot.com/2018/01/tokenization-dalam-bahasa-inggris.html\n",
    "<br>\n",
    "http://www.yasirblog.com/2017/05/normalisasi-data-text-text.html\n",
    "<br>\n",
    "http://norvig.com/spell-correct.html\n",
    "<br>\n",
    "https://github.com/sastrawi/nlp-bahasa-indonesia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py_translator import Translator\n",
    "import enchant\n",
    "\n",
    "def translate(text):\n",
    "    counter = 1;\n",
    "    checker = enchant.Dict(\"en_US\")\n",
    "    split_text = text.split(' ')\n",
    "    words = []\n",
    "    translator = Translator()\n",
    "    for word in split_text:\n",
    "        if word != '':\n",
    "            if checker.check(word) and word != 'twitter':\n",
    "                translated_word = translator.translate(text=word, dest='id', src='en').text \n",
    "                words.append(translated_word)\n",
    "                if counter > 10:\n",
    "                    delay_request(5,7)\n",
    "                    counter = 1\n",
    "            else:\n",
    "                words.append(word.replace(' ',''))\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "import random\n",
    "\n",
    "def delay_request(lower, upper):\n",
    "    print(random.uniform(lower, upper))\n",
    "    sleep(random.uniform(lower, upper))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_url(text):\n",
    "    return re.sub(r'http\\S+', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_text_to_lowercase(text):\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(text):\n",
    "    return re.sub(r'(^|\\s)(:D|:\\/|:\\)+|;\\)|:-\\))(?=\\s|[^[:alnum:]+-]|$)', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def substitue_character(text):\n",
    "    new_text = text.encode('ascii',errors='ignore').decode('utf-8', 'ignore')\n",
    "    new_text = re.sub(r'[.,\\/#!$%\\^&\\*;:{}=\\-_`~()]', ' ', new_text)\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_to_many_character(text):\n",
    "    list_of_char = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
    "    list_of_vocal = ['a', 'i', 'u', 'e', 'o']\n",
    "    new_text = text\n",
    "    for idx in range(len(list_of_vocal)):\n",
    "        for jdx in range(idx, len(list_of_vocal)):\n",
    "            pattern = r'\\b' + list_of_vocal[idx] + list_of_vocal[jdx] + r'\\b'\n",
    "            pattern_2 = r'\\b' + list_of_vocal[jdx] + list_of_vocal[idx] + r'\\b'\n",
    "            new_text = re.sub(pattern, '', new_text)\n",
    "            new_text = re.sub(pattern_2, '', new_text)\n",
    "    for character in list_of_char:\n",
    "        pattern = character + r'{3,}'\n",
    "        pattern_2 = character + r'{2,}\\b'\n",
    "        new_text = re.sub(pattern, character, new_text)\n",
    "        new_text = re.sub(pattern_2, character, new_text)\n",
    "    new_text = re.sub(r'(wk){2,}', '', new_text)\n",
    "    new_text = re.sub(r'(ha){2,}', '', new_text)\n",
    "    new_text = re.sub(r'(he){2,}', '', new_text)\n",
    "    new_text = re.sub(r'(hi){2,}', '', new_text)\n",
    "    new_text = re.sub(r'\\b[a-z]\\b', '', new_text)\n",
    "    new_text = re.sub(r'\\b(kw)\\b', '', new_text)\n",
    "    new_text = re.sub(r'\\b(ha)\\b', '', new_text)\n",
    "    new_text = re.sub(r'\\b(he)\\b', '', new_text)\n",
    "    new_text = re.sub(r'\\b(hi)\\b', '', new_text)\n",
    "    new_text = re.sub(r' {2,}', ' ', new_text)\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "\n",
    "def remove_stop_words(sentence):\n",
    "    factory = StopWordRemoverFactory().create_stop_word_remover()\n",
    "    return factory.remove(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "def stemming(sentence):\n",
    "    factory = StemmerFactory()\n",
    "    stemmer = factory.create_stemmer()\n",
    "    return stemmer.stem(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    new_text = substitue_character(text)\n",
    "    new_text = convert_text_to_lowercase(new_text)\n",
    "    new_text = remove_url(new_text)\n",
    "    new_text = remove_emoji(new_text)\n",
    "    new_text = cut_to_many_character(new_text)\n",
    "#     new_text = translate(new_text)\n",
    "    new_text = remove_stop_words(new_text)\n",
    "    new_text = stemming(new_text)\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_train['tweet'] = df_tweets_train['tweet'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ramai tagar jangansuriahkanindonesia hubung suriah dgn indonesia sih biar aksibelatauhid211 aksi islamselamatkannegeri'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets_train['tweet'].iloc[200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lagi ramai Tagar \\n\\n#JanganSuriahkanIndonesia  ..\\n\\nApa hubungannya Suriah dgn Indonesia? \\nMaksudnya apa sih..?\\n\\nBiarlah mereka berkata apa \\n#AKSIBELATAUHID211  aksi nyata\\n#IslamSelamatkanNegeri  \\nItu PASTI!!!'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets['tweet'].iloc[200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = {}\n",
    "for text in df_tweets_train['tweet']:\n",
    "    split_text = text.split(' ')\n",
    "    for word in split_text:\n",
    "        dummy[word] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('uniquejson.json', '+w') as opened_file:\n",
    "    json.dump(dummy, opened_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
