{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main goal:\n",
    " - summary the indonesia text using machine learning approach\n",
    "\n",
    "Dataset:\n",
    " - twitter\n",
    " - news\n",
    " - blogger\n",
    "\n",
    "Steps:\n",
    " 1. pre-processing:\n",
    "    - colletion data and cleansing the data\n",
    "    - reduce the problem dimensionality (stop words, stemming, features selection, etc.)\n",
    " 2. dataset modeling: \n",
    "    - deciding how to construct training datadatset. \n",
    " 3. Analysis: \n",
    "    - summary indonesian text using one or more algorithm. many options: Reinforcement Algorithm (Jiwanggi & Adriani, 2016), k-nn, SVM, Naive Bayes, Neural network\n",
    "\n",
    "\n",
    "Referencess:\n",
    " Summary indonesian text\n",
    "  - Meganingrum Arista Jiwanggi, Mirna Adriani. 2016, \"Topic Summarization of Microblog Document in Bahasa Indonesia using the Phrase Reinforcement Algorithm\"\n",
    "\n",
    " Pre-processing models (OPTIONS)\n",
    "  - Mirna Adriani, Jelita Asian, Bobby Nazief, Seyed MM Tahaghoghi, Hugh E Williams. 2007, \"Stemming Indonesian: A confix-stripping approach\" \n",
    "  - Mirna Adriani. 2000, \"Using statistical term similarity for sense disambiguation in cross-language information retrieval\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Komponen yang dibuat sendiri:\n",
    "1. Wordnet (Sinonim)\n",
    "2. NLG (buat ngerangkai kalimat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "path = './dataset'\n",
    "dataset = []\n",
    " \n",
    "directories = os.listdir(path)\n",
    "for dir_name in directories:\n",
    "    path_file = path + '/' + dir_name\n",
    "    files = os.listdir(path_file)\n",
    "    for file_name in files:\n",
    "        path_to_open = path_file + '/' + file_name\n",
    "        with open(path_to_open, 'r') as f:\n",
    "            tweet_info = {}\n",
    "            read_file = json.loads(f.read())\n",
    "            tweet_info['trending_topic'] = dir_name\n",
    "            tweet_info['tweet'] = read_file['text']\n",
    "            dataset.append(tweet_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataset(dataset):\n",
    "    dataset.to_csv('twitter_data.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1857, 2)"
      ]
     },
     "execution_count": 517,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_tweets = pd.DataFrame(dataset)\n",
    "df_tweets_train = df_tweets.copy()\n",
    "df_tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "#JanganSuriahkanIndonesia    1857\n",
       "Name: trending_topic, dtype: int64"
      ]
     },
     "execution_count": 518,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets['trending_topic'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# JanganSuriahkanIndonesia'"
      ]
     },
     "execution_count": 519,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets['tweet'].iloc[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing\n",
    "https://github.com/riochr17/Named-Entity-Tagger-ID\n",
    "<br>\n",
    "https://github.com/lilia-simeonova/preprocessing-tweets\n",
    "<br>\n",
    "http://iopscience.iop.org/article/10.1088/1742-6596/801/1/012072/pdf\n",
    "<br>\n",
    "http://taufiksutanto.blogspot.com/2018/01/tokenization-dalam-bahasa-inggris.html\n",
    "<br>\n",
    "http://www.yasirblog.com/2017/05/normalisasi-data-text-text.html\n",
    "<br>\n",
    "http://norvig.com/spell-correct.html\n",
    "<br>\n",
    "https://github.com/sastrawi/nlp-bahasa-indonesia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py_translator import Translator\n",
    "import enchant\n",
    "\n",
    "def translate(text):\n",
    "    counter = 1;\n",
    "    checker = enchant.Dict(\"en_US\")\n",
    "    split_text = text.split(' ')\n",
    "    words = []\n",
    "    translator = Translator()\n",
    "    for word in split_text:\n",
    "        if word != '':\n",
    "            if checker.check(word) and word != 'twitter':\n",
    "                translated_word = translator.translate(text=word, dest='id', src='en').text \n",
    "                words.append(translated_word)\n",
    "                if counter > 10:\n",
    "                    delay_request(5,7)\n",
    "                    counter = 1\n",
    "            else:\n",
    "                words.append(word.replace(' ',''))\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "import random\n",
    "\n",
    "def delay_request(lower, upper):\n",
    "    print(random.uniform(lower, upper))\n",
    "    sleep(random.uniform(lower, upper))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_url(text):\n",
    "    return re.sub(r'http\\S+', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_text_to_lowercase(text):\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(text):\n",
    "    return re.sub(r'(^|\\s)(:D|:\\/|:\\)+|;\\)|:-\\))(?=\\s|[^[:alnum:]+-]|$)', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def spell_checker(text):\n",
    "    words = text.split(' ')\n",
    "    with open('spellchecker.json') as file_opened:\n",
    "        text = file_opened.read()\n",
    "    word_checker = json.loads(text)\n",
    "    for idx in range(len(words)):\n",
    "        if words[idx] in word_checker:\n",
    "            words[idx] = word_checker[words[idx]]\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "def substitue_character(text):\n",
    "    new_text = text.encode('ascii',errors='ignore').decode('utf-8', 'ignore')\n",
    "    new_text = new_text.replace('\\n', ' ')\n",
    "    new_text = new_text.replace('?', ' ')\n",
    "    new_text = new_text.replace('\"', '')\n",
    "    new_text = new_text.replace(\"'\", '')\n",
    "    new_text = re.sub(r'[.,\\/!$%\\^&\\*;:{}=\\`~()]', ' ', new_text)\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dwilingga(text):\n",
    "    match = re.match(r'\\w*(?=2(nya)?)', text)\n",
    "    if match is not None:\n",
    "        words = text.split(' ')\n",
    "        for idx in range(len(words)):\n",
    "            match_word = re.match(r'\\w*(?=2(nya)?)', words[idx])\n",
    "            if match_word is not None:\n",
    "                words[idx] = words[idx].replace('2', '-' + match_word.group())\n",
    "        return \" \".join(words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_code_word(text):\n",
    "    match = re.search(r'(?:[\\d]+[\\w]|[\\w]+[\\d])[\\w\\d]*', text)\n",
    "    if match is not None:\n",
    "        words = text.split(' ')\n",
    "        for idx in range(len(words)):\n",
    "            match_word = re.match(r'(?:[\\d]+[\\w]|[\\w]+[\\d])[\\w\\d]*', words[idx])\n",
    "            match_hastag = re.match(r'(#[\\w\\d]*)', words[idx])\n",
    "            if match_word is not None and match_hastag is None:\n",
    "                words[idx] = ''\n",
    "        return \" \".join(words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prefix_fixing(text):\n",
    "    match = re.search(r'\\b[bkmpst][bcdfghjklmnpqrstvwxyz]', text)\n",
    "    words = text.split(' ')\n",
    "    if match is not None:\n",
    "        for idx in range(len(words)):\n",
    "            match_word = re.search(r'\\b[bkmpst][bcdfghjklmnpqrstvwxyz]', words[idx])\n",
    "            if match_word is not None:\n",
    "                words[idx] = words[idx][0] + 'e' + words[idx][1:]\n",
    "    match3 = re.search(r'\\b[p][bcdefghjklmnpqstvwxyz]', text)\n",
    "    if match3 is not None:\n",
    "        for idx in range(len(words)):\n",
    "            match_word = re.search(r'\\b[p][bcdfghjklmnpqstvwxyz]', words[idx])\n",
    "            if match_word is not None:\n",
    "                words[idx] = words[idx][0] + 'i' + words[idx][1:]\n",
    "    match2 = re.search(r'\\b[d][bcdefghjklmnpqrstvwxyz]', text)\n",
    "    if match2 is not None:\n",
    "        for idx in range(len(words)):\n",
    "            match_word = re.search(r'\\b[d][bcdfghjklmnpqrstvwxyz]', words[idx])\n",
    "            if match_word is not None:\n",
    "                words[idx] = words[idx][0] + 'i' + words[idx][1:]\n",
    "    return \" \".join(words) if match is not None or match2 is not None or match3 is not None else text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_word(text):\n",
    "    match = re.match(r'(?!ke)\\d*\\b', text)\n",
    "    number_dict = {\n",
    "        '1': 'satu',\n",
    "        '2': 'dua',\n",
    "        '3': 'tiga',\n",
    "        '4': 'empat',\n",
    "        '5': 'lima',\n",
    "        '6': 'enam',\n",
    "        '7': 'tujuh',\n",
    "        '8': 'delapan',\n",
    "        '9': 'sembilan'\n",
    "    }\n",
    "    if match is not None:\n",
    "        words = text.split(' ')\n",
    "        for idx in range(len(words)):\n",
    "            match_word = re.match(r'(?!ke)\\d*\\b', words[idx])\n",
    "            if match_word is not None:\n",
    "                words[idx] = words[idx].replace('2', '-' + match_word.group())\n",
    "        return \" \".join(words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_mention(text):\n",
    "    return re.sub(r'@[\\w\\d_]+', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_to_many_character(text):\n",
    "    list_of_char = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
    "    list_of_vocal = ['a', 'i', 'u', 'e', 'o']\n",
    "    new_text = text\n",
    "    for idx in range(len(list_of_vocal)):\n",
    "        for jdx in range(idx, len(list_of_vocal)):\n",
    "            pattern = r'\\b' + list_of_vocal[idx] + list_of_vocal[jdx] + r'\\b'\n",
    "            pattern_2 = r'\\b' + list_of_vocal[jdx] + list_of_vocal[idx] + r'\\b'\n",
    "            pattern_3 = r'\\b[bcdfghjklmnpqrstvwxyz][bcdfghjklmnpqrstvwxyz]\\b'\n",
    "            new_text = re.sub(pattern, '', new_text)\n",
    "            new_text = re.sub(pattern_2, '', new_text)\n",
    "            new_text = re.sub(pattern_3, '', new_text)\n",
    "    for character in list_of_char:\n",
    "        pattern = character + r'{3,}'\n",
    "        pattern_2 = character + r'{2,}\\b'\n",
    "        new_text = re.sub(pattern, character, new_text)\n",
    "        new_text = re.sub(pattern_2, character, new_text)\n",
    "    new_text = re.sub(r'(wk){2,}', '', new_text)\n",
    "    new_text = re.sub(r'(ha){2,}', '', new_text)\n",
    "    new_text = re.sub(r'(he){2,}', '', new_text)\n",
    "    new_text = re.sub(r'(hi){2,}', '', new_text)\n",
    "    new_text = re.sub(r'(bla){2,}', '', new_text)\n",
    "    new_text = re.sub(r'\\b[a-z]\\b', '', new_text)\n",
    "    new_text = re.sub(r'\\b(kw)\\b', '', new_text)\n",
    "    new_text = re.sub(r'\\b(ha)\\b', '', new_text)\n",
    "    new_text = re.sub(r'\\b(he)\\b', '', new_text)\n",
    "    new_text = re.sub(r'\\b(bla)\\b', '', new_text)\n",
    "    new_text = re.sub(r'\\b(hi)\\b', '', new_text)\n",
    "    new_text = re.sub(r'\\b(co)\\b', '', new_text)\n",
    "    new_text = re.sub(r'\\b(id)\\b', '', new_text)\n",
    "    new_text = re.sub(r'\\b(com)\\b', '', new_text)\n",
    "    new_text = re.sub(r'\\b(org)\\b', '', new_text)\n",
    "    new_text = re.sub(r'\\b(net)\\b', '', new_text)\n",
    "    new_text = re.sub(r'\\b(gov)\\b', '', new_text)\n",
    "    new_text = remove_mention(new_text)\n",
    "    new_text = remove_code_word(new_text)\n",
    "    new_text = dwilingga(new_text)\n",
    "    new_text = rank_word(new_text)\n",
    "    new_text = spell_checker(new_text)\n",
    "    new_text = prefix_fixing(new_text)\n",
    "    new_text = re.sub(r' {2,}', ' ', new_text)\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "\n",
    "def remove_stop_words(sentence):\n",
    "    factory = StopWordRemoverFactory().create_stop_word_remover()\n",
    "    return factory.remove(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "def stemming(sentence):\n",
    "    factory = StemmerFactory()\n",
    "    stemmer = factory.create_stemmer()\n",
    "    return stemmer.stem(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    new_text = substitue_character(text)\n",
    "    new_text = convert_text_to_lowercase(new_text)\n",
    "    new_text = remove_url(new_text)\n",
    "    new_text = remove_emoji(new_text)\n",
    "    new_text = cut_to_many_character(new_text)\n",
    "#     new_text = translate(new_text)\n",
    "    new_text = remove_stop_words(new_text)\n",
    "#     new_text = stemming(new_text)\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_train['tweet'] = df_tweets_train['tweet'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# jangansuriahkanindonesia semoga indonesia aman ummat islam semoga bersatu diadu domba nama apapun menyesal negri berantakan #jangansuriahkanindonesia'"
      ]
     },
     "execution_count": 542,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets_train['tweet'].iloc[456]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# JanganSuriahkanIndonesia  ya! Semoga Indonesia tetap aman, ummat Islam semoga tetap bersatu, jangan mau diadu domba atas nama apapun !!! Jangan sampai nanti kita menyesal setelah negri kita berantakan #JanganSuriahkanIndonesia'"
      ]
     },
     "execution_count": 543,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets['tweet'].iloc[456]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = []\n",
    "for text in df_tweets_train['tweet']:\n",
    "    split_text = text.split(' ')\n",
    "    for word in split_text:\n",
    "        dummy.append(word)\n",
    "unique = set(dummy)\n",
    "text_list = {}\n",
    "for word in unique:\n",
    "    text_list[word] =''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('uniquejson.json', '+w') as opened_file:\n",
    "    json.dump(text_list, opened_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'adrian2nya boby jangan2 adrian adrian2'\n",
    "text2 = 'adrian'\n",
    "match = re.match(r'\\w*(?=2(nya)?)', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'adrian'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match.group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = ['a', 'b', 'c']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'b', 'c']"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'str' object does not support item assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-287-8ac818b3665c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'b'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'str' object does not support item assignment"
     ]
    }
   ],
   "source": [
    "test[0] = 'b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'meereka'"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefix_fixing('mereka')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
