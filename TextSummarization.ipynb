{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "path = './dataset'\n",
    "dataset = []\n",
    "\n",
    "directories = os.listdir(path)\n",
    "for dir_name in directories:\n",
    "    path_file = path + '/' + dir_name\n",
    "    files = os.listdir(path_file)\n",
    "    for file_name in files:\n",
    "        path_to_open = path_file + '/' + file_name\n",
    "        with open(path_to_open, 'r') as f:\n",
    "            tweet_info = {}\n",
    "            read_file = json.loads(f.read())\n",
    "            tweet_info['trending_topic'] = dir_name\n",
    "            tweet_info['tweet'] = read_file['text']\n",
    "            dataset.append(tweet_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataset(dataset):\n",
    "    dataset.to_csv('twitter_data.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2458, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_tweets = pd.DataFrame(dataset)\n",
    "df_tweets_train = df_tweets.copy()\n",
    "df_tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "#PesonaAngklungFestival    2458\n",
       "Name: trending_topic, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets['trending_topic'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Kontrak Ramsey bakal abis di akhir musim ini. Ayo tim pencari pemain gratisan siap2 rebutan. Kira2 tim manakah yg beruntung?? :)\\n.\\n#ramsey  #arsenal  #free  #transfer  #kontrak  #gratis  #like4like  #comment4comment  #likeme  #vacation  #PressconMissWorld2018  #PesonaAngklungFestival   pic.twitter.com/wB6G6wf6nB'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets['tweet'].iloc[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing\n",
    "https://github.com/riochr17/Named-Entity-Tagger-ID\n",
    "<br>\n",
    "https://github.com/lilia-simeonova/preprocessing-tweets\n",
    "<br>\n",
    "http://iopscience.iop.org/article/10.1088/1742-6596/801/1/012072/pdf\n",
    "<br>\n",
    "http://taufiksutanto.blogspot.com/2018/01/tokenization-dalam-bahasa-inggris.html\n",
    "<br>\n",
    "http://www.yasirblog.com/2017/05/normalisasi-data-text-text.html\n",
    "<br>\n",
    "http://norvig.com/spell-correct.html\n",
    "<br>\n",
    "https://github.com/sastrawi/nlp-bahasa-indonesia\n",
    "<br>\n",
    "https://towardsdatascience.com/overview-of-text-similarity-metrics-3397c4601f50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py_translator import Translator\n",
    "import enchant\n",
    "\n",
    "def translate(text):\n",
    "    counter = 1;\n",
    "    checker = enchant.Dict(\"en_US\")\n",
    "    split_text = text.split(' ')\n",
    "    words = []\n",
    "    translator = Translator()\n",
    "    for word in split_text:\n",
    "        if word != '':\n",
    "            if checker.check(word) and word != 'twitter':\n",
    "                translated_word = translator.translate(text=word, dest='id', src='en').text \n",
    "                words.append(translated_word)\n",
    "                if counter > 10:\n",
    "                    delay_request(5,7)\n",
    "                    counter = 1\n",
    "            else:\n",
    "                words.append(word.replace(' ',''))\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "import random\n",
    "\n",
    "def delay_request(lower, upper):\n",
    "    print(random.uniform(lower, upper))\n",
    "    sleep(random.uniform(lower, upper))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_url(text):\n",
    "    return re.sub(r'http\\S+', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_text_to_lowercase(text):\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(text):\n",
    "    return re.sub(r'(^|\\s)(:D|:\\/|:\\)+|;\\)|:-\\))(?=\\s|[^[:alnum:]+-]|$)', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def spell_checker(text):\n",
    "    words = text.split(' ')\n",
    "    with open('spellchecker.json') as file_opened:\n",
    "        text = file_opened.read()\n",
    "    word_checker = json.loads(text)\n",
    "    for idx in range(len(words)):\n",
    "        if words[idx] in word_checker:\n",
    "            words[idx] = word_checker[words[idx]]\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def substitue_character(text):\n",
    "    new_text = text.encode('ascii',errors='ignore').decode('utf-8', 'ignore')\n",
    "    new_text = new_text.replace('\\n', ' ')\n",
    "    new_text = new_text.replace('?', ' ')\n",
    "    new_text = new_text.replace('\"', '')\n",
    "    new_text = new_text.replace(\"'\", '')\n",
    "    new_text = re.sub(r'[.,\\/!$%\\^&\\*;:{}=\\`~()]', ' ', new_text)\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dwilingga(text):\n",
    "    match = re.match(r'\\w*(?=2(nya)?)', text)\n",
    "    if match is not None:\n",
    "        words = text.split(' ')\n",
    "        for idx in range(len(words)):\n",
    "            match_word = re.match(r'\\w*(?=2(nya)?)', words[idx])\n",
    "            if match_word is not None:\n",
    "                words[idx] = words[idx].replace('2', '-' + match_word.group())\n",
    "        return \" \".join(words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_code_word(text):\n",
    "    match = re.search(r'(?:[\\d]+[\\w]|[\\w]+[\\d])[\\w\\d]*', text)\n",
    "    if match is not None:\n",
    "        words = text.split(' ')\n",
    "        for idx in range(len(words)):\n",
    "            match_word = re.match(r'(?:[\\d]+[\\w]|[\\w]+[\\d])[\\w\\d]*', words[idx])\n",
    "            match_hastag = re.match(r'(#[\\w\\d]*)', words[idx])\n",
    "            if match_word is not None and match_hastag is None:\n",
    "                words[idx] = ''\n",
    "        return \" \".join(words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prefix_fixing(text):\n",
    "    match = re.search(r'\\b[bkmpst][bcdfghjklmnpqrstvwxyz]', text)\n",
    "    words = text.split(' ')\n",
    "    if match is not None:\n",
    "        for idx in range(len(words)):\n",
    "            match_word = re.search(r'\\b[bkmpst][bcdfghjklmnpqrstvwxyz]', words[idx])\n",
    "            if match_word is not None:\n",
    "                words[idx] = words[idx][0] + 'e' + words[idx][1:]\n",
    "    match3 = re.search(r'\\b[p][bcdefghjklmnpqstvwxyz]', text)\n",
    "    if match3 is not None:\n",
    "        for idx in range(len(words)):\n",
    "            match_word = re.search(r'\\b[p][bcdfghjklmnpqstvwxyz]', words[idx])\n",
    "            if match_word is not None:\n",
    "                words[idx] = words[idx][0] + 'i' + words[idx][1:]\n",
    "    match2 = re.search(r'\\b[d][bcdefghjklmnpqrstvwxyz]', text)\n",
    "    if match2 is not None:\n",
    "        for idx in range(len(words)):\n",
    "            match_word = re.search(r'\\b[d][bcdfghjklmnpqrstvwxyz]', words[idx])\n",
    "            if match_word is not None:\n",
    "                words[idx] = words[idx][0] + 'i' + words[idx][1:]\n",
    "    return \" \".join(words) if match is not None or match2 is not None or match3 is not None else text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_word(text):\n",
    "    match = re.match(r'(?!ke)\\d*\\b', text)\n",
    "    number_dict = {\n",
    "        '1': 'satu',\n",
    "        '2': 'dua',\n",
    "        '3': 'tiga',\n",
    "        '4': 'empat',\n",
    "        '5': 'lima',\n",
    "        '6': 'enam',\n",
    "        '7': 'tujuh',\n",
    "        '8': 'delapan',\n",
    "        '9': 'sembilan'\n",
    "    }\n",
    "    if match is not None:\n",
    "        words = text.split(' ')\n",
    "        for idx in range(len(words)):\n",
    "            match_word = re.match(r'(?!ke)\\d*\\b', words[idx])\n",
    "            if match_word is not None:\n",
    "                words[idx] = words[idx].replace('2', '-' + match_word.group())\n",
    "        return \" \".join(words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_mention(text):\n",
    "    return re.sub(r'@[\\w\\d_]+', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_to_many_character(text):\n",
    "    list_of_char = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
    "    list_of_vocal = ['a', 'i', 'u', 'e', 'o']\n",
    "    new_text = text\n",
    "    for idx in range(len(list_of_vocal)):\n",
    "        for jdx in range(idx, len(list_of_vocal)):\n",
    "            pattern = r'\\b' + list_of_vocal[idx] + list_of_vocal[jdx] + r'\\b'\n",
    "            pattern_2 = r'\\b' + list_of_vocal[jdx] + list_of_vocal[idx] + r'\\b'\n",
    "            pattern_3 = r'\\b[bcdfghjklmnpqrstvwxyz][bcdfghjklmnpqrstvwxyz]\\b'\n",
    "            new_text = re.sub(pattern, '', new_text)\n",
    "            new_text = re.sub(pattern_2, '', new_text)\n",
    "            new_text = re.sub(pattern_3, '', new_text)\n",
    "    for character in list_of_char:\n",
    "        pattern = character + r'{3,}'\n",
    "        pattern_2 = character + r'{2,}\\b'\n",
    "        new_text = re.sub(pattern, character, new_text)\n",
    "        new_text = re.sub(pattern_2, character, new_text)\n",
    "    new_text = re.sub(r'(wk){2,}', '', new_text)\n",
    "    new_text = re.sub(r'(ha){2,}', '', new_text)\n",
    "    new_text = re.sub(r'(he){2,}', '', new_text)\n",
    "    new_text = re.sub(r'(hi){2,}', '', new_text)\n",
    "    new_text = re.sub(r'(bla){2,}', '', new_text)\n",
    "    new_text = re.sub(r'\\b[a-z]\\b', '', new_text)\n",
    "    new_text = re.sub(r'\\b(kw)\\b', '', new_text)\n",
    "    new_text = re.sub(r'\\b(ha)\\b', '', new_text)\n",
    "    new_text = re.sub(r'\\b(he)\\b', '', new_text)\n",
    "    new_text = re.sub(r'\\b(bla)\\b', '', new_text)\n",
    "    new_text = re.sub(r'\\b(hi)\\b', '', new_text)\n",
    "    new_text = re.sub(r'\\b(co)\\b', '', new_text)\n",
    "    new_text = re.sub(r'\\b(id)\\b', '', new_text)\n",
    "    new_text = re.sub(r'\\b(com)\\b', '', new_text)\n",
    "    new_text = re.sub(r'\\b(org)\\b', '', new_text)\n",
    "    new_text = re.sub(r'\\b(net)\\b', '', new_text)\n",
    "    new_text = re.sub(r'\\b(gov)\\b', '', new_text)\n",
    "    new_text = remove_mention(new_text)\n",
    "    new_text = remove_code_word(new_text)\n",
    "    new_text = dwilingga(new_text)\n",
    "    new_text = rank_word(new_text)\n",
    "    new_text = prefix_fixing(new_text)\n",
    "    new_text = re.sub(r' {2,}', ' ', new_text)\n",
    "    new_text = re.sub(r'twitter', '', new_text)\n",
    "    new_text = re.sub(r'tewitter', '', new_text)\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "\n",
    "def remove_stop_words(sentence):\n",
    "    factory = StopWordRemoverFactory().create_stop_word_remover()\n",
    "    return factory.remove(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "def stemming(sentence):\n",
    "    factory = StemmerFactory()\n",
    "    stemmer = factory.create_stemmer()\n",
    "    return stemmer.stem(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    new_text = substitue_character(text)\n",
    "    new_text = convert_text_to_lowercase(new_text)\n",
    "    new_text = remove_url(new_text)\n",
    "    new_text = remove_emoji(new_text)\n",
    "    new_text = cut_to_many_character(new_text)\n",
    "#     new_text = translate(new_text)\n",
    "    new_text = remove_stop_words(new_text)\n",
    "    new_text = spell_checker(new_text)\n",
    "    new_text = stemming(new_text)\n",
    "    new_text = new_text.replace('#', '')\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_train['tweet'] = df_tweets_train['tweet'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_frequencies_list(tweets):\n",
    "    word_frequencies = []\n",
    "    word_dict = []\n",
    "    for tweet in tweets:\n",
    "        words_in_sentence = tweet.split(' ')\n",
    "        for word in words_in_sentence:\n",
    "            if word in word_dict:\n",
    "                word_frequencies[word_dict.index(word)] += 1\n",
    "            else:\n",
    "                word_frequencies.append(1)\n",
    "                word_dict.append(word)\n",
    "    return word_dict, word_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_word_list(word_dict, word_frequencies, treshold=2):\n",
    "    new_word_frequencies = word_frequencies.copy()\n",
    "    new_word_dict = word_dict.copy()\n",
    "    idx = 0\n",
    "    while idx < len(new_word_frequencies):\n",
    "        if new_word_frequencies[idx] < treshold:\n",
    "            new_word_dict.pop(idx)\n",
    "            new_word_frequencies.pop(idx)\n",
    "        else:\n",
    "            idx += 1\n",
    "    return new_word_dict, new_word_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_frequencies_list_all(tweets):\n",
    "    graph = []\n",
    "    graph_idx = 0\n",
    "    for tweet in tweets:\n",
    "        words_in_sentence = tweet.split(' ')\n",
    "        prev_idx = -1\n",
    "        idx_now = 0\n",
    "        for word in words_in_sentence:\n",
    "            if word != '':\n",
    "                is_not_in_graph = True\n",
    "                for idx_graph in range(len(graph)):\n",
    "                    if graph[idx_graph]['word'] == word:\n",
    "                        graph[idx_graph]['counter'] += 1\n",
    "                        is_not_in_graph = False\n",
    "                        idx_graph_representation = idx_graph\n",
    "                        break\n",
    "                if is_not_in_graph:\n",
    "                    node = {\n",
    "                        'idx': graph_idx,\n",
    "                        'word': word,\n",
    "                        'counter': 1,\n",
    "                        'next_neighbours': [],\n",
    "                        'prev_neighbours': []\n",
    "                    }\n",
    "                    if prev_idx != -1:\n",
    "                        node['prev_neighbours'].append(prev_idx)\n",
    "                        if graph_idx not in graph[prev_idx]['next_neighbours'] and graph_idx != prev_idx:\n",
    "                            graph[prev_idx]['next_neighbours'].append(graph_idx)\n",
    "                    prev_idx = graph_idx\n",
    "                    graph_idx += 1\n",
    "                    graph.append(node)\n",
    "                else:\n",
    "                    if prev_idx != -1:\n",
    "                        if prev_idx not in graph[idx_graph_representation]['prev_neighbours'] and idx_graph_representation != prev_idx:\n",
    "                            graph[idx_graph_representation]['prev_neighbours'].append(prev_idx)\n",
    "                        if idx_graph_representation not in graph[prev_idx]['next_neighbours'] and idx_graph_representation != prev_idx:\n",
    "                            graph[prev_idx]['next_neighbours'].append(idx_graph_representation)\n",
    "                    prev_idx = idx_graph_representation\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_frequencies_list_reduce(tweets, treshold=2):\n",
    "    words, freqs = word_frequencies_list(tweets)\n",
    "    new_words, new_freqs = reduce_word_list(words, freqs, treshold)\n",
    "    graph = []\n",
    "    graph_idx = 0\n",
    "    for tweet in tweets:\n",
    "        words_in_sentence = tweet.split(' ')\n",
    "        prev_idx = -1\n",
    "        idx_now = 0\n",
    "        for word in words_in_sentence:\n",
    "            if word != '' and word in new_words:\n",
    "                is_not_in_graph = True\n",
    "                for idx_graph in range(len(graph)):\n",
    "                    if graph[idx_graph]['word'] == word:\n",
    "                        graph[idx_graph]['counter'] += 1\n",
    "                        is_not_in_graph = False\n",
    "                        idx_graph_representation = idx_graph\n",
    "                        break\n",
    "                if is_not_in_graph:\n",
    "                    node = {\n",
    "                        'idx': graph_idx,\n",
    "                        'word': word,\n",
    "                        'counter': 1,\n",
    "                        'next_neighbours': [],\n",
    "                        'prev_neighbours': []\n",
    "                    }\n",
    "                    if prev_idx != -1:\n",
    "                        node['prev_neighbours'].append(prev_idx)\n",
    "                        if graph_idx not in graph[prev_idx]['next_neighbours'] and graph_idx != prev_idx:\n",
    "                            graph[prev_idx]['next_neighbours'].append(graph_idx)\n",
    "                    prev_idx = graph_idx\n",
    "                    graph_idx += 1\n",
    "                    graph.append(node)\n",
    "                else:\n",
    "                    if prev_idx != -1:\n",
    "                        if prev_idx not in graph[idx_graph_representation]['prev_neighbours'] and idx_graph_representation != prev_idx:\n",
    "                            graph[idx_graph_representation]['prev_neighbours'].append(prev_idx)\n",
    "                        if idx_graph_representation not in graph[prev_idx]['next_neighbours'] and idx_graph_representation != prev_idx:\n",
    "                            graph[prev_idx]['next_neighbours'].append(idx_graph_representation)\n",
    "                    prev_idx = idx_graph_representation\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def calculate_weight(counter, distance, base):\n",
    "    if distance == 0:\n",
    "        return counter - (distance * (math.log(counter)/math.log(base)))\n",
    "    return (counter - (distance * (math.log(counter)/math.log(base)))) / distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_path(idx, graph, distance, base, data={'path': [], 'weight': 0}, iterator=5):\n",
    "    if len(graph[idx]['next_neighbours']) == 0 or iterator == distance:\n",
    "        new_data = {\n",
    "            'path': data['path'].copy(),\n",
    "            'weight': data['weight']\n",
    "        }\n",
    "        if idx not in data['path']:\n",
    "            new_data['path'].append(idx)\n",
    "            new_data['weight'] += calculate_weight(graph[idx]['counter'], distance, base)\n",
    "        return new_data\n",
    "    else:\n",
    "        new_data = {\n",
    "            'path': data['path'].copy(),\n",
    "            'weight': data['weight']\n",
    "        }\n",
    "        if idx not in data['path']:\n",
    "            new_data['path'].append(idx)\n",
    "            new_data['weight'] += calculate_weight(graph[idx]['counter'], distance, base)\n",
    "            neighbours = []\n",
    "            for next_idx in graph[idx]['next_neighbours']:\n",
    "                data_send = {\n",
    "                    'path': new_data['path'].copy(),\n",
    "                    'weight': data['weight']\n",
    "                }\n",
    "                if next_idx not in data_send['path']:\n",
    "                    neighbours.append(get_next_path(next_idx, graph, distance + 1, base, data_send, iterator))\n",
    "            data_max = {'path': [], 'weight': 0}\n",
    "            for neighbour in neighbours:\n",
    "                if neighbour['weight'] > data_max['weight']:\n",
    "                    data_max = neighbour.copy()\n",
    "            return data_max.copy()\n",
    "        else:\n",
    "            return new_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prev_path(idx, graph, distance, base, data={'path': [], 'weight': 0}, iterator=7):\n",
    "    if len(graph[idx]['next_neighbours']) == 0 or iterator == distance:\n",
    "        new_data = {\n",
    "            'path': data['path'].copy(),\n",
    "            'weight': data['weight']\n",
    "        }\n",
    "        if idx not in data['path']:\n",
    "            new_data['path'].insert(0, idx)\n",
    "            new_data['weight'] += calculate_weight(graph[idx]['counter'], distance, base)\n",
    "        return new_data\n",
    "    else:\n",
    "        new_data = {\n",
    "            'path': data['path'].copy(),\n",
    "            'weight': data['weight']\n",
    "        }\n",
    "        if idx not in data['path']:\n",
    "            new_data['path'].insert(0, idx)\n",
    "            new_data['weight'] += calculate_weight(graph[idx]['counter'], distance, base)\n",
    "            neighbours = []\n",
    "            for next_idx in graph[idx]['prev_neighbours']:\n",
    "                data_send = {\n",
    "                    'path': new_data['path'].copy(),\n",
    "                    'weight': data['weight']\n",
    "                }\n",
    "                if next_idx not in data_send['path']:\n",
    "                    neighbours.append(get_prev_path(next_idx, graph, distance + 1, base, data_send, iterator))\n",
    "            data_max = {'path': [], 'weight': 0}\n",
    "            for neighbour in neighbours:\n",
    "                if neighbour['weight'] > data_max['weight']:\n",
    "                    data_max = neighbour.copy()\n",
    "            return data_max.copy()\n",
    "        else:\n",
    "            return new_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_index(graph, topic):\n",
    "    for node in graph:\n",
    "        if node['word'] == topic:\n",
    "            return node['idx']\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_path(path_next, path_prev):\n",
    "    all_path = {\n",
    "        'path': path_prev['path'][:-1] + path_next['path'],\n",
    "        'weight': path_prev['weight'] + path_next['weight']\n",
    "    }\n",
    "    return all_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence(graph, path):\n",
    "    sentence = ''\n",
    "    for item in path['path']:\n",
    "        for node in graph:\n",
    "            if node['idx'] == item:\n",
    "                sentence += node['word'] + ' '\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(tweets, topic, base=2, iterator=5, treshold=10):\n",
    "    graph = graph_frequencies_list_reduce(tweets, treshold)\n",
    "    topic_index = get_topic_index(graph, topic)\n",
    "    if topic_index == -1:\n",
    "        return 'this string cannot be a topic'\n",
    "    path_next = get_next_path(topic_index, graph, 0, base, iterator=iterator)\n",
    "    path_prev = get_prev_path(topic_index, graph, 0, base, iterator=iterator)\n",
    "    all_path = combine_path(path_next, path_prev)\n",
    "    return get_sentence(graph, all_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "trendinc_topic = df_tweets_train['trending_topic'].iloc[0].replace('#', '').lower()\n",
    "sentence = summarize(df_tweets_train['tweet'], trendinc_topic, treshold=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "angklung salah waris budaya mancanegara pesonaangklungfestival serta masyarakat sunda event angklung \n"
     ]
    }
   ],
   "source": [
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words_dict(sentence, tweet):\n",
    "    list_of_word_sentence = sentence.split(' ')\n",
    "    list_of_word_tweet = tweet.split(' ')\n",
    "    list_word = set(list_of_word_sentence) | set(list_of_word_tweet)\n",
    "    data = []\n",
    "    dict_sentence = {}\n",
    "    dict_tweet = {}\n",
    "    for word in list_word:\n",
    "        if word not in dict_sentence:\n",
    "            dict_sentence[word] = 0\n",
    "        if word not in dict_tweet:\n",
    "            dict_tweet[word] = 0\n",
    "    for word in list_of_word_sentence:\n",
    "        dict_sentence[word] += 1\n",
    "    for word in list_of_word_tweet:\n",
    "        dict_tweet[word] += 1\n",
    "    data.append(dict_sentence)\n",
    "    data.append(dict_tweet)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words_array(sentence, tweet):\n",
    "    list_of_word_sentence = sentence.split(' ')\n",
    "    list_of_word_tweet = tweet.split(' ')\n",
    "    list_word = set(list_of_word_sentence) | set(list_of_word_tweet)\n",
    "    dict_sentence = []\n",
    "    dict_tweet = []\n",
    "    for word in list_word:\n",
    "        dict_sentence.append(list_of_word_sentence.count(word))\n",
    "        dict_tweet.append(list_of_word_tweet.count(word))\n",
    "    return [dict_sentence, dict_tweet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import norm\n",
    "\n",
    "def normalize_bag_of_words(vector_word):\n",
    "    norm_vector_word = []\n",
    "    norm_sentence = norm(vector_word[0])\n",
    "    norm_tweet = norm(vector_word[1])\n",
    "    dict_sentence = [value * norm_sentence / len(vector_word[0]) for value in vector_word[0]]\n",
    "    dict_tweet = [value * norm_tweet / len(vector_word[1]) for value in vector_word[1]]\n",
    "    return [dict_sentence, dict_tweet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_similarity(vector_word):\n",
    "    sum = 0\n",
    "    for idx in range(len(vector_word[0])):\n",
    "        sum += (vector_word[0][idx] * vector_word[1][idx])\n",
    "    return sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarity(sentence, tweets):\n",
    "    data = []\n",
    "    for tweet in tweets:\n",
    "        data.append(count_similarity(normalize_bag_of_words(bag_of_words_array(sentence, tweet))))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets['similarity'] = get_similarity(sentence, df_tweets_train['tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dalam event itu angklung juga akan bersanding dengan kebudayaan mancanegara. Salah satunya dengan kebudayaan Jepang. Ini sangat bagus untuk mempertegas jika angklung adalah warisan budaya Indonesia\\n\\n #PesonaAngklungFestival'"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets['tweet'][df_tweets['similarity'] == max(df_tweets['similarity'])].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "angklung salah waris budaya mancanegara pesonaangklungfestival serta masyarakat sunda event angklung \n"
     ]
    }
   ],
   "source": [
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite_sentence(sentence, tweet):\n",
    "    word_in_sentence = sentence.split(' ')\n",
    "    word_in_tweet = tweet.lower().split(' ')\n",
    "    new_sentence = []\n",
    "    for word in word_in_sentence:\n",
    "        if word != '':\n",
    "            for word_tw in word_in_tweet:\n",
    "                if word in word_tw:\n",
    "                    new_sentence.append(word_tw)\n",
    "                    break\n",
    "    return \" \".join(new_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'angklung salah warisan kebudayaan mancanegara. #pesonaangklungfestival event angklung'"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewrite_sentence(sentence, df_tweets['tweet'][df_tweets['similarity'] == max(df_tweets['similarity'])].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
