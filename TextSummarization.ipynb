{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main goal:\n",
    " - summary the indonesia text using machine learning approach\n",
    "\n",
    "Dataset:\n",
    " - twitter\n",
    " - news\n",
    " - blogger\n",
    "\n",
    "Steps:\n",
    " 1. pre-processing:\n",
    "    - colletion data and cleansing the data\n",
    "    - reduce the problem dimensionality (stop words, stemming, features selection, etc.)\n",
    " 2. dataset modeling: \n",
    "    - deciding how to construct training datadatset. \n",
    " 3. Analysis: \n",
    "    - summary indonesian text using one or more algorithm. many options: Reinforcement Algorithm (Jiwanggi & Adriani, 2016), k-nn, SVM, Naive Bayes, Neural network\n",
    "\n",
    "\n",
    "Referencess:\n",
    " Summary indonesian text\n",
    "  - Meganingrum Arista Jiwanggi, Mirna Adriani. 2016, \"Topic Summarization of Microblog Document in Bahasa Indonesia using the Phrase Reinforcement Algorithm\"\n",
    "\n",
    " Pre-processing models (OPTIONS)\n",
    "  - Mirna Adriani, Jelita Asian, Bobby Nazief, Seyed MM Tahaghoghi, Hugh E Williams. 2007, \"Stemming Indonesian: A confix-stripping approach\" \n",
    "  - Mirna Adriani. 2000, \"Using statistical term similarity for sense disambiguation in cross-language information retrieval\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "path = './dataset'\n",
    "dataset = []\n",
    " \n",
    "directories = os.listdir(path)\n",
    "for dir_name in directories:\n",
    "    path_file = path + '/' + dir_name\n",
    "    files = os.listdir(path_file)\n",
    "    for file_name in files:\n",
    "        path_to_open = path_file + '/' + file_name\n",
    "        with open(path_to_open, 'r') as f:\n",
    "            tweet_info = {}\n",
    "            read_file = json.loads(f.read())\n",
    "            tweet_info['trending_topic'] = dir_name\n",
    "            tweet_info['tweet'] = read_file['text']\n",
    "            dataset.append(tweet_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28396, 2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_tweets = pd.DataFrame(dataset)\n",
    "df_tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "#2018MAMA                    16610\n",
       "#BLACKPINK2019WORLDTOUR       5205\n",
       "#PesonaAngklungFestival       2458\n",
       "#JanganSuriahkanIndonesia     1857\n",
       "#WelcomeGenpiSulbar           1750\n",
       "#NUMuhammadiyahSepakat         516\n",
       "Name: trending_topic, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets['trending_topic'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Genpi selalu mmbuat trending topic di twitter, ngepost dstinasi di Instagram dan Facebook, menulis berita pariwisata di blog media dan platform  http:// Genpi.co \\xa0  \\n\\n.\\nDew\\n.\\n#WelcomeGenpiSulbar'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets['tweet'].iloc[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_characters(text):\n",
    "    new_text = text.replace('\\n', '')\n",
    "    new_text = new_text.replace('\\xa0', '')\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "\n",
    "def remove_stop_words(sentence):\n",
    "    factory = StopWordRemoverFactory().create_stop_word_remover()\n",
    "    return factory.remove(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "def stemming(sentence):\n",
    "    factory = StemmerFactory()\n",
    "    stemmer = factory.create_stemmer()\n",
    "    return stemmer.stem(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets['tweet'] = df_tweets['tweet'].apply(remove_special_characters)\n",
    "df_tweets['tweet'] = df_tweets['tweet'].apply(remove_stop_words)\n",
    "df_tweets['tweet'] = df_tweets['tweet'].apply(stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets['tweet'].iloc[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
